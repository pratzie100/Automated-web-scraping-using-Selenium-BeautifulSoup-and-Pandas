{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52d19b28-6df1-4a2f-8bd9-70dd27f7e718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing product row 1\n",
      "Scraping from Mainstreet...\n",
      "Scraping from Crepdogcrew...\n",
      "Scraping from Hypefly...\n",
      "Scraping from Culture-circle...\n",
      "\n",
      "Processing product row 2\n",
      "Scraping from Mainstreet...\n",
      "Scraping from Crepdogcrew...\n",
      "Scraping from Hypefly...\n",
      "Scraping from Culture-circle...\n",
      "Basic data scraping complete. Output saved to 'DATA_ANALYSIS_1.csv'.\n",
      "Added product_no and is_best_price column. Output saved to 'DATA_ANALYSIS_2.csv'.\n",
      "FINAL BEST PRICES OBTAINED FOR EVERY UNIQUE SIZE PER PRODUCT. Output saved to 'FINAL_BEST_PRICES.csv'.\n"
     ]
    }
   ],
   "source": [
    "################### IMPORTS ######################\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait, Select\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "################### SCRAPER FUNCTIONS ######################\n",
    "\n",
    "def scrape_mainstreet_product(url: str) -> pd.DataFrame:\n",
    "    # Set Chrome options\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "    # Initialize WebDriver\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "\n",
    "    def scroll_to_bottom(driver):\n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        while True:\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(2)\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                break\n",
    "            last_height = new_height\n",
    "\n",
    "    def get_price(soup):\n",
    "        price_tag = soup.find(\"span\", class_=\"price-item--sale\")\n",
    "        if not price_tag:\n",
    "            price_tag = soup.find(\"span\", class_=\"price-item--regular\")\n",
    "        price = price_tag.text.strip() if price_tag else \"N/A\"\n",
    "        return price\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        scroll_to_bottom(driver)\n",
    "\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, \"product__title\"))\n",
    "        )\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        # Title\n",
    "        title_tag = soup.find(\"h2\", class_=\"h1\")\n",
    "        title = title_tag.text.strip() if title_tag else \"N/A\"\n",
    "\n",
    "        # Description\n",
    "        desc_tag = soup.find(\"div\", class_=\"product__description\")\n",
    "        description = desc_tag.get_text(strip=True) if desc_tag else \"N/A\"\n",
    "\n",
    "        # Sizes and Prices\n",
    "        size_price_mapping = {}\n",
    "        try:\n",
    "            size_select = WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.CSS_SELECTOR, \"select.select__select\"))\n",
    "            )\n",
    "            select = Select(size_select)\n",
    "\n",
    "            options = select.options\n",
    "            available_sizes = [opt.get_attribute(\"value\") for opt in options if opt.get_attribute(\"value\") and \"Unavailable\" not in opt.text]\n",
    "\n",
    "            for size in available_sizes:\n",
    "                retries = 2\n",
    "                for attempt in range(retries):\n",
    "                    try:\n",
    "                        size_select = WebDriverWait(driver, 10).until(\n",
    "                            EC.element_to_be_clickable((By.CSS_SELECTOR, \"select.select__select\"))\n",
    "                        )\n",
    "                        select = Select(size_select)\n",
    "\n",
    "                        driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", size_select)\n",
    "                        time.sleep(1)\n",
    "                        select.select_by_value(size)\n",
    "\n",
    "                        time.sleep(3)\n",
    "                        WebDriverWait(driver, 15).until(\n",
    "                            EC.presence_of_element_located((By.CSS_SELECTOR, \"span.price-item--sale, span.price-item--regular\"))\n",
    "                        )\n",
    "\n",
    "                        updated_soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "                        price = get_price(updated_soup)\n",
    "                        size_price_mapping[size] = price\n",
    "                        break\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"[ERROR] Attempt {attempt + 1} failed for size {size}: {str(e)}\")\n",
    "                        if attempt == retries - 1:\n",
    "                            size_price_mapping[size] = \"Error\"\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Size dropdown not found: {str(e)}\")\n",
    "            price = get_price(soup)\n",
    "            size_price_mapping[\"N/A\"] = price\n",
    "\n",
    "        if not size_price_mapping:\n",
    "            size_price_mapping = {\"N/A\": \"N/A\"}\n",
    "\n",
    "        # Images\n",
    "        try:\n",
    "            WebDriverWait(driver, 20).until(\n",
    "                EC.presence_of_all_elements_located((By.XPATH, \"//ul[contains(@id, 'Slider-Thumbnails')]//li//button//img\"))\n",
    "            )\n",
    "            image_elements = driver.find_elements(By.XPATH, \"//ul[contains(@id, 'Slider-Thumbnails')]//li//button//img\")\n",
    "            images = []\n",
    "\n",
    "            for img in image_elements:\n",
    "                src = img.get_attribute(\"src\")\n",
    "                if src.startswith(\"//\"):\n",
    "                    src = \"https:\" + src\n",
    "                images.append(src)\n",
    "\n",
    "            if not images:\n",
    "                raise Exception(\"Primary thumbnail images not found\")\n",
    "\n",
    "        except Exception:\n",
    "            try:\n",
    "                fallback_img = driver.find_element(By.XPATH, \"//div[contains(@class, 'product__media') and contains(@class, 'media--transparent')]//img\")\n",
    "                fallback_src = fallback_img.get_attribute(\"src\")\n",
    "                if fallback_src.startswith(\"//\"):\n",
    "                    fallback_src = \"https:\" + fallback_src\n",
    "                images = [fallback_src]\n",
    "            except Exception as fallback_error:\n",
    "                print(f\"Fallback image fetch failed: {fallback_error}\")\n",
    "                images = [\"N/A\"]\n",
    "\n",
    "        # Prepare DataFrame\n",
    "        rows = []\n",
    "        for size, price in size_price_mapping.items():\n",
    "            rows.append({\n",
    "                \"title\": title,\n",
    "                \"url\": url,\n",
    "                \"size\": size,\n",
    "                \"price\": price,\n",
    "                \"description\": description,\n",
    "                \"sku\": \"N/A\",\n",
    "                \"images\": ', '.join(images)\n",
    "            })\n",
    "\n",
    "        df = pd.DataFrame(rows)\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to scrape {url}: {str(e)}\")\n",
    "        return pd.DataFrame([{\n",
    "            \"title\": \"Error\",\n",
    "            \"url\": url,\n",
    "            \"size\": \"N/A\",\n",
    "            \"price\": \"Error\",\n",
    "            \"description\": \"Error\",\n",
    "            \"sku\": \"N/A\",\n",
    "            \"images\": \"Error\"\n",
    "        }])\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "def scrape_hypefly_product(url: str) -> pd.DataFrame:\n",
    "    options = Options()\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(\"--window-size=1920,1080\")\n",
    "    \n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "        time.sleep(2)\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "        # Title\n",
    "        title = soup.find(\"h1\").get_text(strip=True) if soup.find(\"h1\") else \"N/A\"\n",
    "\n",
    "        # SKU & Description\n",
    "        try:\n",
    "            info_box = soup.find(\"div\", class_=\"bg-gray-200\")\n",
    "            sku_elem = info_box.find(\"p\", string=lambda x: x and x.strip().startswith(\"SKU:\"))\n",
    "            sku = sku_elem.get_text(strip=True).split(\"SKU:\")[1].strip() if sku_elem else \"N/A\"\n",
    "            description = info_box.find(\"div\", class_=\"staticPage\").get_text(strip=True)\n",
    "        except:\n",
    "            sku, description = \"N/A\", \"N/A\"\n",
    "\n",
    "        # Image\n",
    "        try:\n",
    "            img_tag = soup.find(\"img\", {\"alt\": lambda x: x and title in x})\n",
    "            img_src = img_tag[\"src\"]\n",
    "            img_url = \"https://hypefly.co.in\" + img_src\n",
    "        except:\n",
    "            img_url = \"N/A\"\n",
    "\n",
    "        # Size dropdown\n",
    "        try:\n",
    "            size_btn = wait.until(EC.element_to_be_clickable((By.XPATH, \"//button[.//p[text()='Size:']]\")))\n",
    "            ActionChains(driver).move_to_element(size_btn).click().perform()\n",
    "            time.sleep(2)\n",
    "        except Exception as e:\n",
    "            print(\"Failed to click Size dropdown:\", e)\n",
    "\n",
    "        # Refresh soup\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        size_divs = soup.select(\"ul.grid li\")\n",
    "\n",
    "        size_price_dict = {}\n",
    "        for div in size_divs:\n",
    "            try:\n",
    "                parts = list(div.stripped_strings)\n",
    "                size = parts[0]\n",
    "                price = parts[1] if len(parts) > 1 else \"N/A\"\n",
    "                size_price_dict[size] = price\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        # Build DataFrame\n",
    "        rows = [{\n",
    "            \"title\": title,\n",
    "            \"url\": url,\n",
    "            \"size\": size,\n",
    "            \"price\": price,\n",
    "            \"description\": description,\n",
    "            \"sku\": sku,\n",
    "            \"images\": img_url\n",
    "        } for size, price in size_price_dict.items()]\n",
    "\n",
    "        return pd.DataFrame(rows)\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "def scrape_crepdogcrew_product(url: str) -> pd.DataFrame:\n",
    "    options = Options()\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"--window-size=1920,1080\")\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    try:\n",
    "        # --- Scroll to Bottom ---\n",
    "        def scroll_to_bottom():\n",
    "            last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            while True:\n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                time.sleep(2)\n",
    "                new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                if new_height == last_height:\n",
    "                    break\n",
    "                last_height = new_height\n",
    "\n",
    "        # --- Extract Price from Soup ---\n",
    "        def get_price(soup):\n",
    "            tag = soup.select_one(\"sale-price span.cvc-money\")\n",
    "            return tag.get_text(strip=True).replace(\"MRP\", \"\").strip() if tag else \"N/A\"\n",
    "\n",
    "        # --- Begin Scraping ---\n",
    "        driver.get(url)\n",
    "        scroll_to_bottom()\n",
    "        WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.CLASS_NAME, \"product-info__title\")))\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        title = soup.find(\"h1\", class_=\"product-info__title\")\n",
    "        title = title.get_text(strip=True) if title else \"N/A\"\n",
    "\n",
    "        desc_container = soup.find(\"div\", class_=\"accordion__content\")\n",
    "        description, sku = \"N/A\", \"N/A\"\n",
    "        if desc_container:\n",
    "            prose = desc_container.find(\"div\", class_=\"prose\")\n",
    "            if prose:\n",
    "                desc_parts = []\n",
    "                for p in prose.find_all(\"p\"):\n",
    "                    text = p.get_text(strip=True)\n",
    "                    if \"SKU\" in text.upper():\n",
    "                        match = re.search(r\"SKU\\s*[-:–]?\\s*(.+)\", text, re.I)\n",
    "                        if match:\n",
    "                            sku = match.group(1).strip()\n",
    "                    else:\n",
    "                        desc_parts.append(text)\n",
    "                description = \" \".join(desc_parts).strip()\n",
    "\n",
    "        size_price_mapping = {}\n",
    "        size_grid_found = False\n",
    "        for fieldset in soup.find_all(\"fieldset\", class_=\"variant-picker__option\"):\n",
    "            legend = fieldset.find(\"legend\")\n",
    "            if legend and \"size\" in legend.get_text(strip=True).lower():\n",
    "                size_grid_found = True\n",
    "                for label in fieldset.select(\"label.block-swatch:not(.is-disabled)\"):\n",
    "                    span = label.find(\"span\")\n",
    "                    size = span.get_text(strip=True) if span else \"N/A\"\n",
    "                    input_id = label.get(\"for\")\n",
    "                    if input_id:\n",
    "                        try:\n",
    "                            btn = driver.find_element(By.ID, input_id)\n",
    "                            driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", btn)\n",
    "                            time.sleep(1)\n",
    "                            driver.execute_script(\"arguments[0].click();\", btn)\n",
    "                            time.sleep(3)\n",
    "                            updated_soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "                            price = get_price(updated_soup)\n",
    "                            size_price_mapping[size] = price\n",
    "                        except:\n",
    "                            size_price_mapping[size] = \"Error\"\n",
    "\n",
    "        if not size_grid_found:\n",
    "            size_price_mapping = {\"N/A\": get_price(soup)}\n",
    "\n",
    "        images = []\n",
    "        for img in soup.select(\"page-dots img\"):\n",
    "            src = img.get(\"src\")\n",
    "            if src and src.startswith(\"//\"):\n",
    "                images.append(\"https:\" + src)\n",
    "\n",
    "        if not images:\n",
    "            img = soup.select_one(\"div.product-gallery__media.snap-center.is-selected img\")\n",
    "            if img:\n",
    "                src = img.get(\"src\")\n",
    "                if src and src.startswith(\"//\"):\n",
    "                    images.append(\"https:\" + src)\n",
    "\n",
    "        if not images:\n",
    "            images = [\"N/A\"]\n",
    "\n",
    "        return pd.DataFrame([{\n",
    "            \"title\": title,\n",
    "            \"url\": url,\n",
    "            \"size\": size,\n",
    "            \"price\": price,\n",
    "            \"description\": description,\n",
    "            \"sku\": sku,\n",
    "            \"images\": ', '.join(images)\n",
    "        } for size, price in size_price_mapping.items()])\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "def scrape_culture_circle_product(url: str) -> pd.DataFrame:\n",
    "    options = Options()\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"--start-maximized\")\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "    try:\n",
    "        # --- Scroll to Bottom ---\n",
    "        def scroll_to_bottom():\n",
    "            last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            while True:\n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                time.sleep(1.5)\n",
    "                new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                if new_height == last_height:\n",
    "                    break\n",
    "                last_height = new_height\n",
    "\n",
    "        # --- Begin Scraping ---\n",
    "        driver.get(url)\n",
    "        scroll_to_bottom()\n",
    "\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, \"a_productHeading__jLymj\"))\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            read_more = driver.find_element(By.XPATH, \"//button[contains(text(), 'Read more')]\")\n",
    "            driver.execute_script(\"arguments[0].click();\", read_more)\n",
    "            time.sleep(1)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        title_div = soup.find(\"div\", class_=\"a_productHeading__jLymj\")\n",
    "        product_name = title_div.find(\"h2\").text.strip() if title_div and title_div.find(\"h2\") else \"N/A\"\n",
    "        title = product_name\n",
    "\n",
    "        desc_tag = soup.find(\"p\", class_=\"w-full\")\n",
    "        description = desc_tag.get_text(strip=True) if desc_tag else \"N/A\"\n",
    "\n",
    "        image_tags = soup.find_all(\"img\", class_=\"a_thumbnailImage___06oR\")\n",
    "        images = [img['src'] for img in image_tags if img.get('src')]\n",
    "\n",
    "        # Updated Image Extraction\n",
    "        images = []\n",
    "        # Try primary image source (a_thumbnailImage___06oR)\n",
    "        image_tags = soup.find_all(\"img\", class_=\"a_thumbnailImage___06oR\")\n",
    "        images = [img['src'] for img in image_tags if img.get('src')]\n",
    "\n",
    "        # Fallback to a_mainImage__kjiv_ if no images found\n",
    "        if not images:\n",
    "            fallback_image = soup.find(\"div\", class_=\"a_imageWrapper__fi6Ev\")\n",
    "            if fallback_image:\n",
    "                img_tag = fallback_image.find(\"img\", class_=\"a_mainImage__kjiv_\")\n",
    "                if img_tag and img_tag.get('src'):\n",
    "                    images.append(img_tag['src'])\n",
    "\n",
    "        # If still no images, set a default placeholder\n",
    "        if not images:\n",
    "            images = [\"N/A\"]\n",
    "        \n",
    "        sizes_prices = []\n",
    "        seen = set()\n",
    "        size_wrappers = soup.find_all(\"div\", class_=\"a_sizeSlide__FHiSL\")\n",
    "        for size_div in size_wrappers:\n",
    "            size = size_div.find(\"div\", class_=\"a_sizeSlideSize__jBG1p\")\n",
    "            price = size_div.find(\"p\", class_=\"a_sizeSlidePrice__NASxX\")\n",
    "            if size and price:\n",
    "                size_text = size.get_text(strip=True)\n",
    "                price_text = price.get_text(strip=True)\n",
    "                if (size_text, price_text) not in seen:\n",
    "                    seen.add((size_text, price_text))\n",
    "                    sizes_prices.append({\"size\": size_text, \"price\": price_text})\n",
    "\n",
    "        if not sizes_prices:\n",
    "            sizes_prices = [{\"size\": \"N/A\", \"price\": \"N/A\"}]\n",
    "\n",
    "        df = pd.DataFrame([{\n",
    "            \"title\": title,\n",
    "            \"url\": url,\n",
    "            \"size\": sp[\"size\"],\n",
    "            \"price\": sp[\"price\"],\n",
    "            \"description\": description,\n",
    "            \"sku\": \"N/A\",\n",
    "            \"images\": \", \".join(images) if images else \"N/A\"\n",
    "        } for sp in sizes_prices])\n",
    "\n",
    "        return df\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "################### MAIN EXECUTION ######################\n",
    "\n",
    "# Load the links CSV\n",
    "df = pd.read_csv('links fr testing price tool - Sheet1.csv')\n",
    "\n",
    "# Normalize column names\n",
    "df.columns = df.columns.str.strip().str.lower().str.replace(\" \", \"-\")\n",
    "\n",
    "# Count the number of URL columns (brands) in the input CSV\n",
    "url_columns = [col for col in df.columns if df[col].apply(lambda x: isinstance(x, str) and x.startswith(\"http\")).any()]\n",
    "num_brands = len(url_columns)\n",
    "\n",
    "df.rename(columns={\n",
    "    \"mainstreet\": \"mainstreet\",\n",
    "    \"crepdogcrew\": \"crepdogcrew\",\n",
    "    \"hyepfly\": \"hypefly\",\n",
    "    \"culture-circle\": \"culture-circle\"\n",
    "}, inplace=True)\n",
    "\n",
    "# Holds all the individual product DataFrames\n",
    "all_data = []\n",
    "\n",
    "# Function to choose scraper based on URL\n",
    "def call_scraper(url):\n",
    "    if \"mainstreet\" in url:\n",
    "        return scrape_mainstreet_product(url)\n",
    "    elif \"crepdogcrew\" in url:\n",
    "        return scrape_crepdogcrew_product(url)\n",
    "    elif \"hypefly\" in url:\n",
    "        return scrape_hypefly_product(url)\n",
    "    elif \"culture-circle\" in url:\n",
    "        return scrape_culture_circle_product(url)\n",
    "    else:\n",
    "        print(f\"Unknown source in URL: {url}\")\n",
    "        return None\n",
    "\n",
    "# Go row by row\n",
    "for index, row in df.iterrows():\n",
    "    print(f\"\\nProcessing product row {index + 1}\")\n",
    "    \n",
    "    for col_name in df.columns:\n",
    "        url = row[col_name]\n",
    "        if pd.notna(url) and isinstance(url, str) and url.startswith(\"http\"):\n",
    "            print(f\"Scraping from {col_name.capitalize()}...\")\n",
    "            try:\n",
    "                result_df = call_scraper(url)\n",
    "                if result_df is not None:\n",
    "                    result_df[\"source\"] = col_name.capitalize()\n",
    "                    all_data.append(result_df)\n",
    "            except Exception as e:\n",
    "                print(f\"Error scraping {url}: {e}\")\n",
    "\n",
    "# Final combined output\n",
    "if all_data:\n",
    "    final_df = pd.concat(all_data, ignore_index=True)\n",
    "else:\n",
    "    print(\"\\nNo data scraped. Please check the URLs or scrapers.\")\n",
    "\n",
    "#data cleaning\n",
    "final_df['price'] = final_df['price'].astype(str).str.replace('₹', 'Rs. ', regex=False)\n",
    "def fix_size_format(size):\n",
    "    if not isinstance(size, str):\n",
    "        return size  # leave non-string values (like NaN or pd.NA) untouched\n",
    "    return re.sub(r'^(UK|EU|US)\\s*', r'\\1 ', size.strip(), flags=re.IGNORECASE)\n",
    "final_df['size'] = final_df['size'].apply(fix_size_format) # Standardize the 'size' column\n",
    "\n",
    "final_df.to_csv(\"DATA_ANALYSIS_1.csv\", index=False)\n",
    "print(\"Basic data scraping complete. Output saved to 'DATA_ANALYSIS_1.csv'.\")\n",
    "\n",
    "new_df=final_df.copy()\n",
    "\n",
    "# ADDING NEW COLUMN product_no FOR USING IT AS PRIMARY KEY WITH COLUMN size\n",
    "\n",
    "# Initialize variables\n",
    "current_product_no = 1\n",
    "product_no_list = []\n",
    "seen_brands = set()\n",
    "cycle_started = False\n",
    "first_brand = None\n",
    "\n",
    "# Iterate through new_df row by row\n",
    "for index, row in new_df.iterrows():\n",
    "    url = row['url']\n",
    "    \n",
    "    # Extract brand from URL\n",
    "    if 'mainstreet' in url.lower():\n",
    "        brand = 'mainstreet'\n",
    "    elif 'crepdogcrew' in url.lower():\n",
    "        brand = 'crepdogcrew'\n",
    "    elif 'hypefly' in url.lower():\n",
    "        brand = 'hypefly'\n",
    "    elif 'culture-circle' in url.lower():\n",
    "        brand = 'culture-circle'\n",
    "    else:\n",
    "        brand = 'unknown'\n",
    "    \n",
    "    # Set the first brand encountered\n",
    "    if first_brand is None and brand != 'unknown':\n",
    "        first_brand = brand\n",
    "        \n",
    "    # Check if a new cycle is starting (only on mainstreet after full cycle)\n",
    "    if brand == first_brand and cycle_started and len(seen_brands) >= num_brands:\n",
    "        current_product_no += 1\n",
    "        seen_brands.clear()  # Reset seen_brands for the new cycle\n",
    "        cycle_started = False\n",
    "    \n",
    "    # Add the brand to seen_brands and mark cycle as started\n",
    "    if brand != 'unknown':\n",
    "        seen_brands.add(brand)\n",
    "        cycle_started = True\n",
    "    \n",
    "    # Append the current product_no to the list\n",
    "    product_no_list.append(current_product_no)\n",
    "\n",
    "# Add the 'product_no' column to new_df\n",
    "new_df['product_no'] = product_no_list\n",
    "\n",
    "# Reorder columns to make product_no the first column\n",
    "columns = ['product_no'] + [col for col in new_df.columns if col != 'product_no']\n",
    "new_df = new_df[columns]\n",
    "\n",
    "# Function to clean and convert price to numeric\n",
    "def clean_price(price):\n",
    "    if not isinstance(price, str) or price in ['N/A', 'Error', '']:\n",
    "        return float('inf')  # Use inf for invalid prices to exclude them from min comparison\n",
    "    # Remove currency symbols, commas, and extra spaces\n",
    "    price = re.sub(r'[₹Rs.\\s,]', '', price.strip())\n",
    "    try:\n",
    "        return float(price)\n",
    "    except ValueError:\n",
    "        return float('inf')  # Return inf for non-numeric prices\n",
    "\n",
    "# Apply price cleaning\n",
    "new_df['price_numeric'] = new_df['price'].apply(clean_price)\n",
    "\n",
    "# Initialize is_best_price column\n",
    "new_df['is_best_price'] = False\n",
    "\n",
    "# Group by product_no and size to find the best price\n",
    "for (product_no, size), group in new_df.groupby(['product_no', 'size']):\n",
    "    if size in ['N/A', 'Error', ''] or group['price_numeric'].isna().all():\n",
    "        continue  # Skip invalid sizes or groups with no valid prices\n",
    "    # Find the minimum price for this (product_no, size) pair\n",
    "    min_price = group['price_numeric'].min()\n",
    "    if min_price == float('inf'):\n",
    "        continue  # Skip if no valid prices in the group\n",
    "    # Mark the row(s) with the minimum price as TRUE\n",
    "    new_df.loc[(new_df['product_no'] == product_no) & \n",
    "               (new_df['size'] == size) & \n",
    "               (new_df['price_numeric'] == min_price), 'is_best_price'] = True\n",
    "\n",
    "# Drop the temporary price_numeric column\n",
    "new_df = new_df.drop(columns=['price_numeric'])\n",
    "\n",
    "new_df.to_csv(\"DATA_ANALYSIS_2.csv\", index=False)\n",
    "print(\"Added product_no and is_best_price column. Output saved to 'DATA_ANALYSIS_2.csv'.\")\n",
    "    \n",
    "# Filter and display rows where is_best_price is True\n",
    "best_price_rows = new_df[new_df['is_best_price'] == True]\n",
    "    \n",
    "# Drop the is_best_price column\n",
    "best_price_rows = best_price_rows.drop(columns=['is_best_price'])\n",
    "\n",
    "best_price_rows.rename(columns={'price': 'best_price', 'source': 'best_seller'}, inplace=True)\n",
    "\n",
    "# Dynamically get the list of companies and append '_price' to each\n",
    "companies = [f\"{company}_price\" for company in new_df['source'].unique().tolist()]\n",
    "\n",
    "# Add columns for each company's price\n",
    "for company in companies:\n",
    "    best_price_rows[company] = '-'\n",
    "\n",
    "# Populate company price columns using DATA_ANALYSIS_2 (new_df)\n",
    "for index, row in best_price_rows.iterrows():\n",
    "    product_no = row['product_no']\n",
    "    size = row['size']\n",
    "        \n",
    "    # Get all prices for this product_no and size from new_df\n",
    "    matching_rows = new_df[(new_df['product_no'] == product_no) & (new_df['size'] == size)]\n",
    "        \n",
    "    for _, match_row in matching_rows.iterrows():\n",
    "        source = match_row['source']\n",
    "        price = match_row['price']\n",
    "        # Map source to the corresponding _price column\n",
    "        source_column = f\"{source}_price\"\n",
    "        if source_column in companies:\n",
    "            best_price_rows.at[index, source_column] = price\n",
    "\n",
    "\n",
    "# Reorder columns to have company prices at the end\n",
    "columns = ['product_no', 'title', 'url', 'size']+ companies + ['best_price', 'best_seller','description', 'sku', 'images'] \n",
    "best_price_rows = best_price_rows[columns]\n",
    "        \n",
    "best_price_rows.to_csv(\"FINAL_BEST_PRICES.csv\", index=False)\n",
    "print(\"FINAL BEST PRICES OBTAINED FOR EVERY UNIQUE SIZE PER PRODUCT. Output saved to 'FINAL_BEST_PRICES.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed14b4c1-6a05-492d-970b-27d400eaf3eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8167d430-858d-4fe5-b905-62fc5401c7ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
